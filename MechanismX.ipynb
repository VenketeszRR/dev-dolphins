import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, current_timestamp
import time
import os
import datetime
from pytz import timezone

# --- Configuration ---
# IMPORTANT: Replace with the actual S3 path where you uploaded transactions.csv and CustomerImportance.csv
# Ensure the 's3a://' prefix for Spark S3 access
INITIAL_SOURCE_S3_PATH_TRANSACTIONS = "s3a://transactions-raw-chunks/transactions.csv"
INITIAL_SOURCE_S3_PATH_CUSTOMER_IMP = "s3a://transactions-raw-chunks/CustomerImportance.csv"

# S3 Path where Mechanism X will write the transaction chunks
# This should be in your transactions-raw-chunks bucket, perhaps in a 'processed_chunks' folder
OUTPUT_CHUNKS_S3_PATH = "s3a://transactions-raw-chunks/chunks/"

CHUNK_SIZE = 1000
INVOCATION_INTERVAL_SECONDS = 7 # Mechanism X invokes every 7 seconds

# Define IST timezone
ist_timezone = timezone('Asia/Kolkata')

# --- Initialize Spark Session ---
spark = SparkSession.builder.appName("MechanismXIngestion").getOrCreate()

# --- Function to read a chunk and write to S3 ---
def process_and_write_chunk(df_transactions, start_row, chunk_id):
    """
    Reads a chunk from the DataFrame, adds metadata, and writes to S3.
    """
    end_row = min(start_row + CHUNK_SIZE, len(df_transactions))
    chunk_df = df_transactions.iloc[start_row:end_row]

    if not chunk_df.empty:
        # Convert pandas DataFrame to Spark DataFrame
        spark_chunk_df = spark.createDataFrame(chunk_df)

        # Add timestamp for when this chunk was processed by Mechanism X in IST
        current_utc_time = datetime.datetime.now(datetime.timezone.utc)
        current_ist_time = current_utc_time.astimezone(ist_timezone)

        # Using lit(current_ist_time) ensures the timestamp is consistent for the whole chunk
        spark_chunk_df = spark_chunk_df.withColumn(
            "ingestion_timestamp_ist",
            lit(current_ist_time.strftime('%Y-%m-%d %H:%M:%S')) # Format as string
        )

        # Define output path for the chunk
        # Use a unique filename for each chunk to avoid overwriting and enable streaming
        output_file_name = f"transactions_chunk_{chunk_id}.csv"
        chunk_output_path = os.path.join(OUTPUT_CHUNKS_S3_PATH, output_file_name)

        # Write the chunk to S3
        print(f"Writing chunk {chunk_id} to {chunk_output_path}...")
        spark_chunk_df.write.mode("overwrite").csv(chunk_output_path, header=True, sep=',')
        print(f"Successfully wrote chunk {chunk_id}.")
        return True
    return False

# --- Main Execution Logic ---
print("Mechanism X: Starting data ingestion simulation from S3...")

# Read the entire transactions.csv using spark.read.csv directly from S3
try:
    spark_transactions_df_full = spark.read.csv(INITIAL_SOURCE_S3_PATH_TRANSACTIONS, header=True, inferSchema=True)
    pdf_transactions = spark_transactions_df_full.toPandas() # Convert to pandas for row-based chunking

    print(f"Successfully loaded {len(pdf_transactions)} rows from {INITIAL_SOURCE_S3_PATH_TRANSACTIONS}.")
except Exception as e:
    print(f"Error loading source CSV from S3: {e}")
    print("Please ensure 'transactions.csv' is uploaded to the specified S3 path and your cluster has correct IAM roles.")
    # Do not use dbutils.notebook.exit() here, as it stops the entire notebook execution.
    # For debugging, it's better to allow the error to surface naturally or use a less aggressive exit.
    raise # Re-raise the exception to clearly show the error

total_rows = len(pdf_transactions)

# Initialize a variable to keep track of the last processed row index
# In a real long-running scenario, this state would be persisted (e.g., in a database)
last_processed_row = 0

# Simple loop to process all chunks
chunk_counter = 0
while last_processed_row < total_rows:
    chunk_counter += 1
    processed = process_and_write_chunk(pdf_transactions, last_processed_row, chunk_counter)

    if processed:
        last_processed_row += CHUNK_SIZE
    else:
        # This should only happen if chunk_df is empty, meaning no more data
        break

    # Simulate invocation every 7 seconds for the next chunk
    if last_processed_row < total_rows:
        print(f"Waiting {INVOCATION_INTERVAL_SECONDS} seconds before processing next chunk...")
        time.sleep(INVOCATION_INTERVAL_SECONDS)

print(f"Mechanism X: Finished processing {chunk_counter} chunks. Total rows processed: {last_processed_row}.")
print("Mechanism X simulation complete.")
