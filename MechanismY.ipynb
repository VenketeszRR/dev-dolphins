from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, current_timestamp, avg, count,
    unix_timestamp, from_unixtime, to_utc_timestamp, from_utc_timestamp,
    row_number, sum as spark_sum, count as spark_count
)
from pyspark.sql.window import Window
from pyspark.sql.types import StringType, IntegerType, FloatType, TimestampType
import datetime
from pytz import timezone
import os
import psycopg2 # Using psycopg2 for direct Python DB interaction

# --- Configuration ---
# S3 Path where Mechanism X writes chunks and Mechanism Y will read from
INPUT_CHUNKS_S3_PATH = "s3a://transactions-raw-chunks/chunks/" # This should match OUTPUT_CHUNKS_S3_PATH from Mechanism X

# S3 Path where Mechanism Y will write detected patterns
# This should be your dedicated output S3 bucket
OUTPUT_DETECTIONS_S3_PATH = "s3a://pattern-detection-dev-dolphins/detection/" # Ensure this is your actual output bucket

# S3 path for customer importance data (static lookup)
CUSTOMER_IMPORTANCE_PATH = "s3a://transactions-raw-chunks/CustomerImportance.csv" # Assuming you put CustomerImportance.csv here too, otherwise adjust

# PostgreSQL Database Credentials (replace with your AWS RDS PostgreSQL endpoint and credentials)
PG_HOST = " " # e.g., my-interview-postgres-db.xxxxxx.ap-south-1.rds.amazonaws.com
PG_PORT = "5432"
PG_DATABASE = " " 
PG_USER = " "
PG_PASSWORD = " "

# Checkpoint location for Structured Streaming (REQUIRED for stateful processing)
# This must be a unique, empty S3 path for each streaming query
CHECKPOINT_LOCATION = "s3a://pattern-detection-dev-dolphins/checkpoints/pattern_detection_stream/"

# Pattern Thresholds
PAT1_MERCHANT_TXN_THRESHOLD = 50000
PAT2_AVG_TXN_VALUE_THRESHOLD = 23.0
PAT2_MIN_TXN_COUNT = 80

# Output batch size for detections
DETECTION_BATCH_SIZE = 50

# Define IST timezone
ist_timezone = timezone('Asia/Kolkata')

# --- Initialize Spark Session ---
spark = SparkSession.builder.appName("MechanismYPatternDetection").getOrCreate()

# --- PostgreSQL Connection Properties (for Spark JDBC - mostly for DDL/reading all) ---
# For writing, we'll use psycopg2 for upserts
pg_url = f"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DATABASE}"
pg_properties = {
    "user": PG_USER,
    "password": PG_PASSWORD,
    "driver": "org.postgresql.Driver"
}

# --- Function to get current IST timestamp ---
def get_current_ist_timestamp_str():
    current_utc_time = datetime.datetime.now(datetime.timezone.utc)
    current_ist_time = current_utc_time.astimezone(ist_timezone)
    return current_ist_time.strftime('%Y-%m-%d %H:%M:%S')

# --- Load Customer Importance Data (static lookup) ---
# Infer schema might be slow for large files, define schema if performance is critical
# IMPORTANT: Ensure CUSTOMER_IMPORTANCE_PATH points to your actual CustomerImportance.csv
try:
    df_customer_importance = spark.read.csv(CUSTOMER_IMPORTANCE_PATH, header=True, inferSchema=True)
    # Cast 'Weight' to Float and ensure 'Source' is customer ID, 'Target' is merchant ID
    df_customer_importance = df_customer_importance.withColumnRenamed("Source", "customer_id") \
                                                   .withColumnRenamed("Target", "merchant_id") \
                                                   .withColumn("weight", col("Weight").cast(FloatType()))
    print(f"Successfully loaded CustomerImportance.csv from {CUSTOMER_IMPORTANCE_PATH}.")
except Exception as e:
    print(f"Error loading CustomerImportance.csv from S3: {e}")
    print("Please ensure 'CustomerImportance.csv' is uploaded to the specified S3 path and your cluster has correct IAM roles.")
    dbutils.notebook.exit("Failed to load Customer Importance data.") # Exit notebook if error


# Calculate percentiles for Customer Importance (PatId1)
# Collect weights to calculate percentiles (careful with very large datasets)
# For very large datasets, use approximate percentiles or sampling.
customer_weights = df_customer_importance.select("weight").rdd.flatMap(lambda x: x).collect()
customer_weights.sort()

# Calculate 1st percentile and 99th percentile for weights
# Note: These will be used for filtering in PatId1
bottom_1_percentile_weight_threshold = customer_weights[int(len(customer_weights) * 0.01)] if customer_weights else 0
top_1_percentile_weight_threshold = customer_weights[int(len(customer_weights) * 0.99)] if customer_weights else float('inf')

print(f"Calculated 1st percentile weight: {bottom_1_percentile_weight_threshold}")
print(f"Calculated 99th percentile weight: {top_1_percentile_weight_threshold}")


# --- Define schema for streaming input (important for CSV) ---
# It's best to define schema explicitly for robustness in streaming CSVs.
# If inferSchema was used in Mechanism X, ensure this matches.

#transaction_schema = SparkSession.builder.getOrCreate().read.csv(INPUT_CHUNKS_S3_PATH, header=True, inferSchema=True).limit(0).schema

# If the above fails (e.g., no files yet), use a hardcoded schema:
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType

# --- Define schema for streaming input (explicitly and directly) ---
# This schema must precisely match the CSV files written by Mechanism X.
# If Mechanism X adds columns or changes types, this schema must be updated.
transaction_schema = StructType([
    StructField("step", IntegerType(), True),
    StructField("customer", StringType(), True),
    StructField("age", StringType(), True), # Age can be 'U'
    StructField("gender", StringType(), True),
    StructField("zipcodeOri", StringType(), True),
    StructField("merchant", StringType(), True),
    StructField("zipMerchant", StringType(), True),
    StructField("category", StringType(), True),
    StructField("amount", FloatType(), True),
    StructField("fraud", IntegerType(), True),
    StructField("ingestion_timestamp_ist", StringType(), True) # Added by Mechanism X
])
print("Using explicitly defined schema for streaming input.")


# Define the streaming DataFrame
streaming_df = spark.readStream \
    .format("csv") \
    .schema(transaction_schema) \
    .option("header", "true") \
    .option("maxFilesPerTrigger", 1) \
    .option("recursiveFileLookup", "true") \
    .load(INPUT_CHUNKS_S3_PATH)

# --- Store/Retrieve Merchant Transaction Counts (for PatId1) in PostgreSQL ---
PG_MERCHANT_COUNTS_TABLE = "merchant_transaction_counts"

# Create table if it doesn't exist using psycopg2
# This needs to be run only once before the stream starts.
try:
    conn = psycopg2.connect(host=PG_HOST, database=PG_DATABASE, user=PG_USER, password=PG_PASSWORD, port=PG_PORT)
    cur = conn.cursor()
    cur.execute(f"""
        CREATE TABLE IF NOT EXISTS {PG_MERCHANT_COUNTS_TABLE} (
            merchant_id VARCHAR(255) PRIMARY KEY,
            total_transactions BIGINT
        );
    """)
    conn.commit()
    print(f"Checked/Created table {PG_MERCHANT_COUNTS_TABLE} in PostgreSQL.")
except Exception as e:
    print(f"ERROR: Could not check/create table {PG_MERCHANT_COUNTS_TABLE} in PostgreSQL. Error: {e}")
    print("Please ensure your RDS instance is reachable, security group allows access, and credentials are correct.")
    dbutils.notebook.exit("Failed to connect to PostgreSQL and create table.")


# UDFs for PostgreSQL interaction within foreachBatch (executed by workers)
# These functions need to establish a new connection for each call, or use a connection pool.
# For a demo, direct connection is often fine, but for high throughput, connection pooling is better.
def upsert_merchant_counts(merchant_id, count_delta):
    """Upserts merchant transaction count in PostgreSQL."""
    try:
        conn = psycopg2.connect(
            host=PG_HOST,
            database=PG_DATABASE,
            user=PG_USER,
            password=PG_PASSWORD,
            port=PG_PORT
        )
        cur = conn.cursor()
        cur.execute(f"""
            INSERT INTO {PG_MERCHANT_COUNTS_TABLE} (merchant_id, total_transactions)
            VALUES (%s, %s)
            ON CONFLICT (merchant_id) DO UPDATE
            SET total_transactions = {PG_MERCHANT_COUNTS_TABLE}.total_transactions + EXCLUDED.total_transactions;
        """, (merchant_id, count_delta))
        conn.commit()
    except Exception as e:
        print(f"Error updating merchant count for {merchant_id}: {e}")
    finally:
        if cur: cur.close()
        if conn: conn.close()

def get_merchant_total_transactions(merchant_id):
    """Retrieves total transactions for a merchant from PostgreSQL."""
    try:
        conn = psycopg2.connect(
            host=PG_HOST,
            database=PG_DATABASE,
            user=PG_USER,
            password=PG_PASSWORD,
            port=PG_PORT
        )
        cur = conn.cursor()
        cur.execute(f"SELECT total_transactions FROM {PG_MERCHANT_COUNTS_TABLE} WHERE merchant_id = %s;", (merchant_id,))
        result = cur.fetchone()
        return result[0] if result else 0
    except Exception as e:
        print(f"Error retrieving merchant count for {merchant_id}: {e}")
        return 0
    finally:
        if cur: cur.close()
        if conn: conn.close()


# --- Processing Logic for Each Micro-Batch ---
def process_batch(current_df, batch_id):
    if current_df.isEmpty():
        print(f"Batch {batch_id} is empty. Skipping processing.")
        return

    y_start_time_ist = get_current_ist_timestamp_str() # Capture YStartTime at the beginning of batch processing
    print(f"Processing batch {batch_id} at {y_start_time_ist}...")

    # Convert ingestion_timestamp_ist to proper timestamp type
    processed_df = current_df.withColumn(
        "ingestion_timestamp_ist",
        to_utc_timestamp(col("ingestion_timestamp_ist"), "Asia/Kolkata").cast(TimestampType())
    )

    # Cache customer importance data for joins
    df_customer_importance.cache()
    df_customer_importance.count() # Trigger cache

    # --- Pattern Detection Logic ---

    # Join with Customer Importance data for PatId1
    transactions_with_importance = processed_df.alias("t") \
        .join(df_customer_importance.alias("ci"),
              (col("t.customer") == col("ci.customer_id")) & (col("t.merchant") == col("ci.merchant_id")),
              "left")

    # --- Update Merchant Transaction Counts (for PatId1 logic) ---
    merchant_counts_in_batch = processed_df.groupBy("merchant").agg(spark_count("*").alias("batch_count"))

    # Collect and update counts in PostgreSQL
    # This is executed on the driver, then calls psycopg2 on worker/driver depending on context.
    # For a high volume of distinct merchants in a batch, consider a more optimized batch upsert.
    merchant_counts_pd = merchant_counts_in_batch.toPandas()
    for index, row in merchant_counts_pd.iterrows():
        upsert_merchant_counts(row['merchant'], row['batch_count'])
    print("Updated merchant transaction counts in PostgreSQL.")

    # --- Apply Pattern 1: UPGRADE ---
    # PatId1: Customer has a weight (from CustomerImportance.csv) in the BOTTOM 1% of overall customer importance weights,
    # AND the merchant with whom the customer transacted has processed more than 50,000 total transactions.
    # Action: UPGRADE

    # Get distinct merchants in this batch to fetch their total counts from PG
    # This is done on the driver and then broadcasted.
    distinct_merchants_in_batch = processed_df.select("merchant").distinct().collect()
    merchant_total_txns_map = {m.merchant: get_merchant_total_transactions(m.merchant) for m in distinct_merchants_in_batch}

    # Broadcast the merchant total transactions map for efficient lookup on workers
    merchant_total_txns_b = spark.sparkContext.broadcast(merchant_total_txns_map)

    # Define a UDF to lookup broadcasted value
    from pyspark.sql.types import LongType
    get_total_txns_udf = spark.udf.register(
        "get_total_txns_udf", # Unique name for UDF
        lambda m_id: merchant_total_txns_b.value.get(m_id, 0),
        LongType()
    )
    pat1_df_prep = transactions_with_importance.withColumn("total_merchant_txns", get_total_txns_udf(col("t.merchant")))

    pat1_detections = pat1_df_prep.filter(
        (col("ci.weight") <= bottom_1_percentile_weight_threshold) & # Customer importance weight is in bottom 1%
        (col("total_merchant_txns") > PAT1_MERCHANT_TXN_THRESHOLD)
    ).select(
        lit("PatId1").alias("patternId"),
        lit("UPGRADE").alias("ActionType"),
        col("t.customer").alias("customerName"),
        col("t.merchant").alias("MerchantId"),
        lit("").alias("YStartTime"), # Placeholder, filled later
        lit("").alias("detectionTime") # Placeholder, filled later
    ).distinct() # Use distinct to avoid duplicate detections if same customer/merchant appears multiple times in a micro-batch

    # --- Apply Pattern 2: CHILD ---
    # PatId2: Customer's average transaction value for a given merchant < Rs 23 and made at least 80 transactions with that merchant.
    # This uses Structured Streaming's stateful aggregation.
    customer_merchant_stats = processed_df.groupBy("customer", "merchant") \
                                        .agg(avg("amount").alias("avg_amount"),
                                             count("*").alias("txn_count"))

    pat2_detections = customer_merchant_stats.filter(
        (col("avg_amount") < PAT2_AVG_TXN_VALUE_THRESHOLD) &
        (col("txn_count") >= PAT2_MIN_TXN_COUNT)
    ).select(
        lit("PatId2").alias("patternId"),
        lit("CHILD").alias("ActionType"),
        col("customer").alias("customerName"),
        col("merchant").alias("MerchantId"),
        lit("").alias("YStartTime"), # Placeholder, filled later
        lit("").alias("detectionTime") # Placeholder, filled later
    ).distinct()


    # --- Apply Pattern 3: DEI-NEEDED ---
    # PatId3: Merchants where Female customers < Male customers overall are marked DEI-NEEDED.
    # This also uses Structured Streaming's stateful aggregation.
    gender_counts_per_merchant = processed_df.filter(col("gender").isin("M", "F")) \
                                            .groupBy("merchant", "gender") \
                                            .agg(spark_count("*").alias("gender_count"))

    pivoted_gender_counts = gender_counts_per_merchant.groupBy("merchant") \
                                                      .pivot("gender", ["M", "F"]) \
                                                      .agg(spark_sum("gender_count")) \
                                                      .fillna(0) # Fill nulls if a gender is missing for a merchant

    pat3_detections = pivoted_gender_counts.filter(
        col("F") < col("M")
    ).select(
        lit("PatId3").alias("patternId"),
        lit("DEI-NEEDED").alias("ActionType"),
        lit("").alias("customerName"), # Not applicable
        col("merchant").alias("MerchantId"),
        lit("").alias("YStartTime"), # Placeholder, filled later
        lit("").alias("detectionTime") # Placeholder, filled later
    ).distinct()


    # --- Union all detections ---
    all_detections = pat1_detections.unionByName(pat2_detections, allowMissingColumns=True) \
                                    .unionByName(pat3_detections, allowMissingColumns=True)

    # Add YStartTime and detectionTime (IST) to all detections
    detection_time_ist_str = get_current_ist_timestamp_str()
    final_detections = all_detections \
        .withColumn("YStartTime", lit(y_start_time_ist)) \
        .withColumn("detectionTime", lit(detection_time_ist_str))

    # --- Write Detections to S3 (50 at a time to a unique file) ---
    if not final_detections.isEmpty():
        num_rows = final_detections.count()
        if num_rows > 0:
            # Calculate number of partitions to get roughly DETECTION_BATCH_SIZE rows per part file
            num_partitions = (num_rows + DETECTION_BATCH_SIZE - 1) // DETECTION_BATCH_SIZE
            num_partitions = max(1, num_partitions) # Ensure at least 1 partition

            # Add a unique folder suffix based on batch_id and a UUID for robustness
            file_uuid = os.urandom(8).hex() # Generate a random string
            output_folder_name = f"detections_batch_{batch_id}_{file_uuid}"
            batch_output_path = os.path.join(OUTPUT_DETECTIONS_S3_PATH, output_folder_name)


            print(f"Writing {num_rows} detections to {batch_output_path} in {num_partitions} parts...")
            final_detections.repartition(num_partitions) \
                            .write.mode("append") \
                            .csv(batch_output_path, header=True)
            print(f"Successfully wrote {num_rows} detections for batch {batch_id}.")
        else:
            print(f"No detections for batch {batch_id}.")
    else:
        print(f"No detections in batch {batch_id}.")

    # Unpersist cached data
    df_customer_importance.unpersist()


# --- Start the Structured Streaming Query ---
print("Mechanism Y: Starting Structured Streaming query...")

query = streaming_df.writeStream \
    .foreachBatch(process_batch) \
    .outputMode("update") \
    .option("checkpointLocation", CHECKPOINT_LOCATION) \
    .trigger(processingTime="10 seconds") \
    .start()

print("Mechanism Y: Structured Streaming query started. Waiting for data...")
query.awaitTermination() # Keep the notebook alive while the stream runs
